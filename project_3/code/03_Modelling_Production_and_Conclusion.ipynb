{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content\n",
    "- [Benchmark Model and Baseline Score](#Benchmark-Model-and-Baseline-Score)\n",
    "- [Hyper-Parameters Tuning](#Hyper-Parameters-Tuning)\n",
    "- [Limitations of Model](#Limitations-of-Model)\n",
    "- [Conclusion and Recommendations](#Conclusion-and-Recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>comments</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RULE REMINDER: You cannot Post Offers to Trade...</td>\n",
       "      <td>Admins have banned other subs for this.\\n\\nNo ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>rule remind cannot post offer trade sell copyr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What are your recommendations on increasing yo...</td>\n",
       "      <td>0</td>\n",
       "      <td>recommend increas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>how about just giving someone a free jump prog...</td>\n",
       "      <td>0</td>\n",
       "      <td>someon jump program origin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>0</td>\n",
       "      <td>delet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>r/Basketball Weekly Discussion: Basketball Sho...</td>\n",
       "      <td>#Welcome to /r/Basketball's weekly Shoe Discus...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>basketbal weekli discuss basketbal shoe septem...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  RULE REMINDER: You cannot Post Offers to Trade...   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4  r/Basketball Weekly Discussion: Basketball Sho...   \n",
       "\n",
       "                                            selftext  \\\n",
       "0  Admins have banned other subs for this.\\n\\nNo ...   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4  #Welcome to /r/Basketball's weekly Shoe Discus...   \n",
       "\n",
       "                                            comments  subreddit  \\\n",
       "0                                                NaN          0   \n",
       "1  What are your recommendations on increasing yo...          0   \n",
       "2  how about just giving someone a free jump prog...          0   \n",
       "3                                          [deleted]          0   \n",
       "4                                                NaN          0   \n",
       "\n",
       "                                                text  \n",
       "0  rule remind cannot post offer trade sell copyr...  \n",
       "1                                  recommend increas  \n",
       "2                         someon jump program origin  \n",
       "3                                              delet  \n",
       "4  basketbal weekli discuss basketbal shoe septem...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Import data\n",
    "combine = pd.read_csv('../datasets/combine.csv')\n",
    "combine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = combine['text']\n",
    "y = combine['subreddit']\n",
    "# X is our feature variable and y is target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title        6502\n",
       "selftext     7306\n",
       "comments     1735\n",
       "subreddit       0\n",
       "text            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_training, X_unseen, y_training, y_unseen = train_test_split(X, y, test_size=0.2, random_state=42, stratify = y)\n",
    "# first split of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_training, y_training, test_size=0.2, random_state=42, stratify = y_training)\n",
    "# second split of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Model and Baseline Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer()\n",
    "X_train_cvec = cvec.fit_transform(X_train)\n",
    "X_test_cvec = cvec.transform(X_test)\n",
    "# Convert a collection of raw documents to a matrix of vectorized features\n",
    "ss = StandardScaler(with_mean = False)\n",
    "X_train_ss = ss.fit_transform(X_train_cvec)\n",
    "X_test_ss = ss.transform(X_test_cvec)\n",
    "# Scale and center vectorized features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark = KNeighborsClassifier()  # Instantiate and fit model\n",
    "benchmark.fit(X_train_ss, y_train)\n",
    "benchmark_predict = benchmark.predict(X_test_ss)  # predicting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mBenchmark model\n",
      "\u001b[0mSpecificity of benchmark\t: 0.6238532110091743\n",
      "Sensitivity of model\t\t: 0.8674698795180723\n",
      "Accuracy for train data\t\t: 0.7427453978074947\n",
      "Accuracy for test data\t\t: 0.6328061988708377\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.62      0.71       654\n",
      "           1       0.70      0.87      0.78       664\n",
      "\n",
      "    accuracy                           0.75      1318\n",
      "   macro avg       0.76      0.75      0.74      1318\n",
      "weighted avg       0.76      0.75      0.74      1318\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Basketball</th>\n",
       "      <th>Predicted Soccer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual Basketball</th>\n",
       "      <td>408</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual Soccer</th>\n",
       "      <td>88</td>\n",
       "      <td>576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Predicted Basketball  Predicted Soccer\n",
       "Actual Basketball                   408               246\n",
       "Actual Soccer                        88               576"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TN, FP, FN, TP = confusion_matrix(y_test,benchmark_predict).ravel()\n",
    "\n",
    "print('\\033[1m'+'Benchmark model')\n",
    "print('\\033[0m'f'Specificity of benchmark\\t: {TN/(TN + FP)}')\n",
    "print(f'Sensitivity of model\\t\\t: {TP/(TP + FN)}')\n",
    "print(f'Accuracy for train data\\t\\t: {cross_val_score(benchmark, X_train_ss, y_train, n_jobs = -1, cv = 5).mean()}')\n",
    "print(f'Accuracy for test data\\t\\t: {cross_val_score(benchmark, X_test_ss, y_test, n_jobs = -1, cv = 5).mean()}\\n')\n",
    "print(classification_report(y_test,benchmark_predict))\n",
    "\n",
    "pd.DataFrame(confusion_matrix(y_test,benchmark_predict),\n",
    "             index = ['Actual Basketball', 'Actual Soccer'],\n",
    "             columns = ['Predicted Basketball','Predicted Soccer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have used the K Nearest Neighbors as our benchmark model. What we are trying to achieve here is to increase the specificity of the model to as close as 1 as possible. We will focus more on specificity as we want to minimize as many false positive as possible. We dont want to have a case where we predicted a sentence/text as soccer but in fact, it is actually basketball. As we have a balanced dataset, We also want to improve our accuracy of the model. Currently, our specificity is at 0.62385 and our accuracy is at 0.63281 to unseen data. We will proceed to tune our parameters to get the best model we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-Parameters Tuning\n",
    "- Multinomial Naive Bayes Model\n",
    "\n",
    "The Count vectorizer will count the number of occurance of each word in each sentence and display the number of words for each column. On the other hand, the Tfidf vectorizer will return the term frequency within each document. There is a possibility of multiple values in each column across different sentences for both vectorizers, so Multinomial Naive Bayes model is the best choice here as it can take in features with multiple variables. Bernoulli Naive Bayes model only takes in features with 0 or 1 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    6.6s\n",
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed:   19.9s finished\n"
     ]
    }
   ],
   "source": [
    "mnb_pipe_cvec = Pipeline([('cvec',CountVectorizer()),('mnb',MultinomialNB())])\n",
    "\n",
    "cvec_params = {'cvec__max_features': [6500, 7500, 8947],\n",
    "    'cvec__min_df': [2, 3],\n",
    "    'cvec__max_df': [0.4, 0.45],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)]}\n",
    "\n",
    "mnb_gs_cvec = GridSearchCV(mnb_pipe_cvec, cvec_params, n_jobs = -1, cv = 5, verbose = 1)\n",
    "mnb_gs_cvec.fit(X_train, y_train)\n",
    "mnb_predict_cvec = mnb_gs_cvec.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mCount vectorizer with MultinomialNB model\n",
      "\u001b[0mSpecificity of model\t\t: 0.9495412844036697\n",
      "Sensitivity of model\t\t: 0.8478915662650602\n",
      "Accuracy for the model\t\t: 0.8911038966878602\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.95      0.90       654\n",
      "           1       0.94      0.85      0.89       664\n",
      "\n",
      "    accuracy                           0.90      1318\n",
      "   macro avg       0.90      0.90      0.90      1318\n",
      "weighted avg       0.90      0.90      0.90      1318\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Basketball</th>\n",
       "      <th>Predicted Soccer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual Basketball</th>\n",
       "      <td>621</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual Soccer</th>\n",
       "      <td>101</td>\n",
       "      <td>563</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Predicted Basketball  Predicted Soccer\n",
       "Actual Basketball                   621                33\n",
       "Actual Soccer                       101               563"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TN, FP, FN, TP = confusion_matrix(y_test,mnb_predict_cvec).ravel()\n",
    "\n",
    "print('\\033[1m'+'Count vectorizer with MultinomialNB model')\n",
    "print('\\033[0m'f'Specificity of model\\t\\t: {TN/(TN + FP)}')\n",
    "print(f'Sensitivity of model\\t\\t: {TP/(TP + FN)}')\n",
    "print(f'Accuracy for the model\\t\\t: {mnb_gs_cvec.best_score_}\\n')\n",
    "print(classification_report(y_test,mnb_predict_cvec))\n",
    "\n",
    "pd.DataFrame(confusion_matrix(y_test,mnb_predict_cvec),\n",
    "             index = ['Actual Basketball', 'Actual Soccer'],\n",
    "             columns = ['Predicted Basketball','Predicted Soccer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words likely related to soccer\t:\n",
      "            Coefficients\n",
      "http          -4.881564\n",
      "player        -4.889017\n",
      "goal          -5.162667\n",
      "match         -5.223496\n",
      "leagu         -5.258961\n",
      "like          -5.258961\n",
      "team          -5.299482\n",
      "substitut     -5.394041\n",
      "would         -5.453633\n",
      "game          -5.521686 \n",
      "\n",
      "Words likely related to basketball\t:\n",
      "               Coefficients\n",
      "zw              -10.882978\n",
      "pivot dribbl    -10.882978\n",
      "pivot           -10.882978\n",
      "pickup game     -10.882978\n",
      "feel better     -10.882978\n",
      "feel comfort    -10.882978\n",
      "feel good       -10.882978\n",
      "feel great      -10.882978\n",
      "physiqu         -10.882978\n",
      "feel pain       -10.882978\n"
     ]
    }
   ],
   "source": [
    "mnb_cv_coef = pd.DataFrame(mnb_gs_cvec.best_estimator_.named_steps['mnb'].coef_,\n",
    "             columns = mnb_gs_cvec.best_estimator_.named_steps['cvec'].get_feature_names()).rename(\n",
    "    index={0:'Coefficients'}).T\n",
    "\n",
    "print('Words likely related to soccer\\t:\\n',mnb_cv_coef.sort_values(by='Coefficients', ascending = False)[:10],'\\n')\n",
    "print('Words likely related to basketball\\t:\\n',mnb_cv_coef.sort_values(by='Coefficients', ascending = True)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    6.8s\n",
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed:   21.7s finished\n"
     ]
    }
   ],
   "source": [
    "mnb_pipe_tf = Pipeline([('tfidf',TfidfVectorizer()),('mnb',MultinomialNB())])\n",
    "\n",
    "tfidf_params = {'tfidf__max_features': [5000, 7500, 10000],\n",
    "    'tfidf__ngram_range': [(1,1), (1,2)],\n",
    "    'tfidf__min_df': [2, 3],\n",
    "    'tfidf__max_df': [0.4, 0.45]}\n",
    "\n",
    "mnb_gs_tf = GridSearchCV(mnb_pipe_tf, tfidf_params, n_jobs = -1, cv = 5, verbose = 1)\n",
    "mnb_gs_tf.fit(X_train, y_train)\n",
    "mnb_predict_tf = mnb_gs_tf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTfidf vectorizer with MultinomialNB model\n",
      "\u001b[0mSpecificity of model\t\t: 0.9235474006116208\n",
      "Sensitivity of model\t\t: 0.8780120481927711\n",
      "Accuracy for the model\t\t: 0.8994492657176003\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.92      0.90       654\n",
      "           1       0.92      0.88      0.90       664\n",
      "\n",
      "    accuracy                           0.90      1318\n",
      "   macro avg       0.90      0.90      0.90      1318\n",
      "weighted avg       0.90      0.90      0.90      1318\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Basketball</th>\n",
       "      <th>Predicted Soccer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual Basketball</th>\n",
       "      <td>604</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual Soccer</th>\n",
       "      <td>81</td>\n",
       "      <td>583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Predicted Basketball  Predicted Soccer\n",
       "Actual Basketball                   604                50\n",
       "Actual Soccer                        81               583"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TN, FP, FN, TP = confusion_matrix(y_test,mnb_predict_tf).ravel()\n",
    "\n",
    "print('\\033[1m'+'Tfidf vectorizer with MultinomialNB model')\n",
    "print('\\033[0m'f'Specificity of model\\t\\t: {TN/(TN + FP)}')\n",
    "print(f'Sensitivity of model\\t\\t: {TP/(TP + FN)}')\n",
    "print(f'Accuracy for the model\\t\\t: {mnb_gs_tf.best_score_}\\n')\n",
    "print(classification_report(y_test,mnb_predict_tf))\n",
    "\n",
    "pd.DataFrame(confusion_matrix(y_test,mnb_predict_tf),\n",
    "             index = ['Actual Basketball', 'Actual Soccer'],\n",
    "             columns = ['Predicted Basketball','Predicted Soccer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words likely related to soccer\t:\n",
      "         Coefficients\n",
      "player     -6.040902\n",
      "sign       -6.143130\n",
      "good       -6.180685\n",
      "like       -6.229467\n",
      "goal       -6.355205\n",
      "season     -6.395407\n",
      "would      -6.411158\n",
      "great      -6.448056\n",
      "well       -6.524089\n",
      "leagu      -6.526777 \n",
      "\n",
      "Words likely related to basketball\t:\n",
      "             Coefficients\n",
      "local team     -9.752477\n",
      "loud           -9.752477\n",
      "lost skill     -9.752477\n",
      "look weird     -9.752477\n",
      "look start     -9.752477\n",
      "look shoe      -9.752477\n",
      "look see       -9.752477\n",
      "look peopl     -9.752477\n",
      "look open      -9.752477\n",
      "look nba       -9.752477\n"
     ]
    }
   ],
   "source": [
    "mnb_tf_coef = pd.DataFrame(mnb_gs_tf.best_estimator_.named_steps['mnb'].coef_,\n",
    "             columns = mnb_gs_tf.best_estimator_.named_steps['tfidf'].get_feature_names()).rename(\n",
    "    index={0:'Coefficients'}).T\n",
    "\n",
    "print('Words likely related to soccer\\t:\\n',mnb_tf_coef.sort_values(by='Coefficients', ascending = False)[:10],'\\n')\n",
    "print('Words likely related to basketball\\t:\\n',mnb_tf_coef.sort_values(by='Coefficients', ascending = True)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compared between Count Vectorizer and Tfidf Vectorizer and tuned its parameters using Grid Search to build our Multinomial Naive Bayes model. Both results have a higher specificity than sensitivity. This shows that our MultinomialNB models are better at predicting texts that are actually basketball than texts that are actually soccer. In comparison to our benchmark model, we managed to get a better Specificity and Accuracy in both MultinomialNB models. We only managed to get better Sensitivity for the MultinomialNB model with Tfidf vectorizer as compared to the benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-Parameters Tuning\n",
    "- Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    9.9s\n",
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed:   27.9s finished\n"
     ]
    }
   ],
   "source": [
    "log_pipe_cvec = Pipeline([('cvec',CountVectorizer()),('logreg', LogisticRegression())])\n",
    "\n",
    "cvec_params = {'cvec__max_features': [6500, 7500, 8947],\n",
    "    'cvec__min_df': [2, 3],\n",
    "    'cvec__max_df': [0.4, 0.45],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)]}\n",
    "\n",
    "log_gs_cvec = GridSearchCV(log_pipe_cvec, cvec_params, n_jobs = -1, cv = 5, verbose = 1)\n",
    "log_gs_cvec.fit(X_train, y_train)\n",
    "log_predict_cvec = log_gs_cvec.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mCount vectorizer with Logistic Regression\n",
      "\u001b[0mSpecificity of model\t\t: 0.8302752293577982\n",
      "Sensitivity of model\t\t: 0.9367469879518072\n",
      "Accuracy for the model\t\t: 0.8893950376359075\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.83      0.88       654\n",
      "           1       0.85      0.94      0.89       664\n",
      "\n",
      "    accuracy                           0.88      1318\n",
      "   macro avg       0.89      0.88      0.88      1318\n",
      "weighted avg       0.89      0.88      0.88      1318\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Basketball</th>\n",
       "      <th>Predicted Soccer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual Basketball</th>\n",
       "      <td>543</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual Soccer</th>\n",
       "      <td>42</td>\n",
       "      <td>622</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Predicted Basketball  Predicted Soccer\n",
       "Actual Basketball                   543               111\n",
       "Actual Soccer                        42               622"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TN, FP, FN, TP = confusion_matrix(y_test,log_predict_cvec).ravel()\n",
    "\n",
    "print('\\033[1m'+'Count vectorizer with Logistic Regression')\n",
    "print('\\033[0m'f'Specificity of model\\t\\t: {TN/(TN + FP)}')\n",
    "print(f'Sensitivity of model\\t\\t: {TP/(TP + FN)}')\n",
    "print(f'Accuracy for the model\\t\\t: {log_gs_cvec.best_score_}\\n')\n",
    "print(classification_report(y_test,log_predict_cvec))\n",
    "\n",
    "pd.DataFrame(confusion_matrix(y_test,log_predict_cvec),\n",
    "             index = ['Actual Basketball', 'Actual Soccer'],\n",
    "             columns = ['Predicted Basketball','Predicted Soccer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words likely related to soccer\t:\n",
      "            Coefficients\n",
      "footbal        1.816624\n",
      "sign           1.796920\n",
      "match          1.677970\n",
      "bayern         1.467118\n",
      "liverpool      1.397566\n",
      "legend         1.381313\n",
      "unit           1.371288\n",
      "striker        1.304146\n",
      "arsen          1.300123\n",
      "goal           1.250824 \n",
      "\n",
      "Words likely related to basketball\t:\n",
      "            Coefficients\n",
      "basketbal     -3.286422\n",
      "nba           -2.774712\n",
      "dunk          -2.431362\n",
      "practic       -2.331687\n",
      "court         -2.020693\n",
      "school        -2.012413\n",
      "jump          -1.939595\n",
      "shoe          -1.939389\n",
      "lebron        -1.893745\n",
      "depend        -1.834010\n"
     ]
    }
   ],
   "source": [
    "log_cv_coef = pd.DataFrame(log_gs_cvec.best_estimator_.named_steps['logreg'].coef_,\n",
    "             columns = log_gs_cvec.best_estimator_.named_steps['cvec'].get_feature_names()).rename(\n",
    "    index={0:'Coefficients'}).T\n",
    "\n",
    "print('Words likely related to soccer\\t:\\n',log_cv_coef.sort_values(by='Coefficients', ascending = False)[:10],'\\n')\n",
    "print('Words likely related to basketball\\t:\\n',log_cv_coef.sort_values(by='Coefficients', ascending = True)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    8.9s\n",
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed:   26.5s finished\n"
     ]
    }
   ],
   "source": [
    "log_pipe_tf = Pipeline([('tfidf',TfidfVectorizer()),('logreg', LogisticRegression())])\n",
    "\n",
    "tfidf_params = {'tfidf__max_features': [5000, 7500, 10000],\n",
    "    'tfidf__ngram_range': [(1,1), (1,2)],\n",
    "    'tfidf__min_df': [2, 3],\n",
    "    'tfidf__max_df': [0.4, 0.45]}\n",
    "\n",
    "log_gs_tf = GridSearchCV(log_pipe_tf, tfidf_params, n_jobs = -1, cv = 5, verbose = 1)\n",
    "log_gs_tf.fit(X_train, y_train)\n",
    "log_predict_tf = log_gs_tf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTfidf vectorizer with Logistic Regression\n",
      "\u001b[0mSpecificity of model\t\t: 0.8440366972477065\n",
      "Sensitivity of model\t\t: 0.9427710843373494\n",
      "Accuracy for the model\t\t: 0.8911013786343158\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.84      0.89       654\n",
      "           1       0.86      0.94      0.90       664\n",
      "\n",
      "    accuracy                           0.89      1318\n",
      "   macro avg       0.90      0.89      0.89      1318\n",
      "weighted avg       0.90      0.89      0.89      1318\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Basketball</th>\n",
       "      <th>Predicted Soccer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual Basketball</th>\n",
       "      <td>552</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual Soccer</th>\n",
       "      <td>38</td>\n",
       "      <td>626</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Predicted Basketball  Predicted Soccer\n",
       "Actual Basketball                   552               102\n",
       "Actual Soccer                        38               626"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TN, FP, FN, TP = confusion_matrix(y_test,log_predict_tf).ravel()\n",
    "\n",
    "print('\\033[1m'+'Tfidf vectorizer with Logistic Regression')\n",
    "print('\\033[0m'f'Specificity of model\\t\\t: {TN/(TN + FP)}')\n",
    "print(f'Sensitivity of model\\t\\t: {TP/(TP + FN)}')\n",
    "print(f'Accuracy for the model\\t\\t: {log_gs_tf.best_score_}\\n')\n",
    "print(classification_report(y_test,log_predict_tf))\n",
    "\n",
    "pd.DataFrame(confusion_matrix(y_test,log_predict_tf),\n",
    "             index = ['Actual Basketball', 'Actual Soccer'],\n",
    "             columns = ['Predicted Basketball','Predicted Soccer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words likely related to soccer\t:\n",
      "            Coefficients\n",
      "sign           3.011367\n",
      "footbal        2.493096\n",
      "match          2.488996\n",
      "season         2.219935\n",
      "goal           2.196048\n",
      "unit           2.188189\n",
      "bayern         2.072980\n",
      "loan           1.903082\n",
      "arsen          1.762674\n",
      "liverpool      1.730885 \n",
      "\n",
      "Words likely related to basketball\t:\n",
      "            Coefficients\n",
      "basketbal     -6.201912\n",
      "nba           -4.471698\n",
      "practic       -3.916437\n",
      "dunk          -3.473481\n",
      "help          -3.392226\n",
      "jump          -3.359188\n",
      "school        -3.164266\n",
      "court         -3.088074\n",
      "work          -3.026016\n",
      "shoe          -2.767702\n"
     ]
    }
   ],
   "source": [
    "log_tf_coef = pd.DataFrame(log_gs_tf.best_estimator_.named_steps['logreg'].coef_,\n",
    "             columns = log_gs_tf.best_estimator_.named_steps['tfidf'].get_feature_names()).rename(\n",
    "    index={0:'Coefficients'}).T\n",
    "\n",
    "print('Words likely related to soccer\\t:\\n',log_tf_coef.sort_values(by='Coefficients', ascending = False)[:10],'\\n')\n",
    "print('Words likely related to basketball\\t:\\n',log_tf_coef.sort_values(by='Coefficients', ascending = True)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compared between Count Vectorizer and Tfidf Vectorizer and tuned its parameters using Grid Search to build our Logistic regression model. Both results have a higher sensitivity than specificity. This shows that our logistic regression models are better at predicting texts that are actually soccer than texts that are actually basketball. Also, we found that Logistic Regression with Tfidf Vectorizer produces a better result than the Logistic Regression with Count Vectorizer. In comparison to our benchmark model, both Logistic Regression models performed better in terms of Specificity, Sensitivity and Accuracy than our benchmarks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-Parameters Tuning\n",
    "- Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  7.4min\n",
      "[Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed:  9.3min finished\n"
     ]
    }
   ],
   "source": [
    "svc_pipe_cvec = Pipeline([('cvec',CountVectorizer()),('ss',StandardScaler()),('svc',SVC())])\n",
    "\n",
    "cvec_params = {'cvec__max_features': [6500, 7500, 8947],\n",
    "    'cvec__min_df': [2, 3],\n",
    "    'cvec__max_df': [0.4, 0.45],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'ss__with_mean': [False],\n",
    "    'svc__C' : [1,10]}\n",
    "\n",
    "svc_gs_cvec = GridSearchCV(svc_pipe_cvec, cvec_params, n_jobs = -1, cv = 5, verbose = 1)\n",
    "svc_gs_cvec.fit(X_train, y_train)\n",
    "svc_predict_cvec = svc_gs_cvec.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mCvec vectorizer with SVC\n",
      "\u001b[0mSpecificity of model\t\t: 0.8042813455657493\n",
      "Sensitivity of model\t\t: 0.9156626506024096\n",
      "Accuracy for the model\t\t: 0.8474665683426712\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.80      0.85       654\n",
      "           1       0.83      0.92      0.87       664\n",
      "\n",
      "    accuracy                           0.86      1318\n",
      "   macro avg       0.86      0.86      0.86      1318\n",
      "weighted avg       0.86      0.86      0.86      1318\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Basketball</th>\n",
       "      <th>Predicted Soccer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual Basketball</th>\n",
       "      <td>526</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual Soccer</th>\n",
       "      <td>56</td>\n",
       "      <td>608</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Predicted Basketball  Predicted Soccer\n",
       "Actual Basketball                   526               128\n",
       "Actual Soccer                        56               608"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TN, FP, FN, TP = confusion_matrix(y_test,svc_predict_cvec).ravel()\n",
    "\n",
    "print('\\033[1m'+'Cvec vectorizer with SVC')\n",
    "print('\\033[0m'f'Specificity of model\\t\\t: {TN/(TN + FP)}')\n",
    "print(f'Sensitivity of model\\t\\t: {TP/(TP + FN)}')\n",
    "print(f'Accuracy for the model\\t\\t: {svc_gs_cvec.best_score_}\\n')\n",
    "print(classification_report(y_test,svc_predict_cvec))\n",
    "\n",
    "pd.DataFrame(confusion_matrix(y_test,svc_predict_cvec),\n",
    "             index = ['Actual Basketball', 'Actual Soccer'],\n",
    "             columns = ['Predicted Basketball','Predicted Soccer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  7.8min\n",
      "[Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed:  9.8min finished\n"
     ]
    }
   ],
   "source": [
    "svc_pipe_tf = Pipeline([('tfidf',TfidfVectorizer()),('ss',StandardScaler()),('svc',SVC())])\n",
    "\n",
    "tfidf_params = {'tfidf__max_features': [5000, 7500, 10000],\n",
    "    'tfidf__ngram_range': [(1,1), (1,2)],\n",
    "    'tfidf__min_df': [2, 3],\n",
    "    'tfidf__max_df': [0.4, 0.45],\n",
    "    'ss__with_mean': [False],\n",
    "    'svc__C' : [1,10]}\n",
    "\n",
    "svc_gs_tf = GridSearchCV(svc_pipe_tf, tfidf_params, n_jobs = -1, cv = 5, verbose = 1)\n",
    "svc_gs_tf.fit(X_train, y_train)\n",
    "svc_predict_tf = svc_gs_tf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTfidf vectorizer with SVC\n",
      "\u001b[0mSpecificity of model\t\t: 0.8119266055045872\n",
      "Sensitivity of model\t\t: 0.9292168674698795\n",
      "Accuracy for the model\t\t: 0.8702335494662625\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.81      0.86       654\n",
      "           1       0.83      0.93      0.88       664\n",
      "\n",
      "    accuracy                           0.87      1318\n",
      "   macro avg       0.88      0.87      0.87      1318\n",
      "weighted avg       0.88      0.87      0.87      1318\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Basketball</th>\n",
       "      <th>Predicted Soccer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual Basketball</th>\n",
       "      <td>531</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual Soccer</th>\n",
       "      <td>47</td>\n",
       "      <td>617</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Predicted Basketball  Predicted Soccer\n",
       "Actual Basketball                   531               123\n",
       "Actual Soccer                        47               617"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TN, FP, FN, TP = confusion_matrix(y_test,svc_predict_tf).ravel()\n",
    "\n",
    "print('\\033[1m'+'Tfidf vectorizer with SVC')\n",
    "print('\\033[0m'f'Specificity of model\\t\\t: {TN/(TN + FP)}')\n",
    "print(f'Sensitivity of model\\t\\t: {TP/(TP + FN)}')\n",
    "print(f'Accuracy for the model\\t\\t: {svc_gs_tf.best_score_}\\n')\n",
    "print(classification_report(y_test,svc_predict_tf))\n",
    "\n",
    "pd.DataFrame(confusion_matrix(y_test,svc_predict_tf),\n",
    "             index = ['Actual Basketball', 'Actual Soccer'],\n",
    "             columns = ['Predicted Basketball','Predicted Soccer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compared between Count Vectorizer and Tfidf Vectorizer and tuned its parameters using Grid Search to build our SVC model. Both results have a higher sensitivity than specificity. This shows that our SVC models are better at predicting texts that are actually soccer than texts that are actually basketball. Also, we found that SVC model with Tfidf Vectorizer produces a better result than the Logistic Regression with Count Vectorizer. In comparison to our benchmark model, both SVC models performed better in terms of Specificity, Sensitivity and Accuracy than our benchmarks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-Parameters Tuning\n",
    "- K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   12.7s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   57.4s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 480 out of 480 | elapsed:  2.4min finished\n"
     ]
    }
   ],
   "source": [
    "knn_pipe_cvec = Pipeline([('cvec',CountVectorizer()),('ss',StandardScaler()),('knn',KNeighborsClassifier())])\n",
    "\n",
    "cvec_params = {'cvec__max_features': [6500, 7500, 8947],\n",
    "    'cvec__min_df': [2, 3],\n",
    "    'cvec__max_df': [0.4, 0.45],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'ss__with_mean': [False],\n",
    "    'knn__n_neighbors':[3,5,7,9]}\n",
    "\n",
    "knn_gs_cvec = GridSearchCV(knn_pipe_cvec, cvec_params, n_jobs = -1, cv = 5, verbose = 1)\n",
    "knn_gs_cvec.fit(X_train, y_train)\n",
    "knn_predict_cvec = knn_gs_cvec.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mCount vectorizer with K Nearest Neighbors\n",
      "\u001b[0mSpecificity of model\t\t: 0.6590214067278287\n",
      "Sensitivity of model\t\t: 0.8298192771084337\n",
      "Accuracy for the model\t\t: 0.7507157567200554\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.66      0.72       654\n",
      "           1       0.71      0.83      0.77       664\n",
      "\n",
      "    accuracy                           0.75      1318\n",
      "   macro avg       0.75      0.74      0.74      1318\n",
      "weighted avg       0.75      0.75      0.74      1318\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Basketball</th>\n",
       "      <th>Predicted Soccer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual Basketball</th>\n",
       "      <td>431</td>\n",
       "      <td>223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual Soccer</th>\n",
       "      <td>113</td>\n",
       "      <td>551</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Predicted Basketball  Predicted Soccer\n",
       "Actual Basketball                   431               223\n",
       "Actual Soccer                       113               551"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TN, FP, FN, TP = confusion_matrix(y_test,knn_predict_cvec).ravel()\n",
    "\n",
    "print('\\033[1m'+'Count vectorizer with K Nearest Neighbors')\n",
    "print('\\033[0m'f'Specificity of model\\t\\t: {TN/(TN + FP)}')\n",
    "print(f'Sensitivity of model\\t\\t: {TP/(TP + FN)}')\n",
    "print(f'Accuracy for the model\\t\\t: {knn_gs_cvec.best_score_}\\n')\n",
    "print(classification_report(y_test,knn_predict_cvec))\n",
    "\n",
    "pd.DataFrame(confusion_matrix(y_test,knn_predict_cvec),\n",
    "             index = ['Actual Basketball', 'Actual Soccer'],\n",
    "             columns = ['Predicted Basketball','Predicted Soccer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   11.4s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   55.8s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 480 out of 480 | elapsed:  2.5min finished\n"
     ]
    }
   ],
   "source": [
    "knn_pipe_tf = Pipeline([('tfidf',TfidfVectorizer()),('ss',StandardScaler()),('knn', KNeighborsClassifier())])\n",
    "\n",
    "tfidf_params = {'tfidf__max_features': [5000, 7500, 10000],\n",
    "    'tfidf__ngram_range': [(1,1), (1,2)],\n",
    "    'tfidf__min_df': [2, 3],\n",
    "    'tfidf__max_df': [0.4, 0.45],\n",
    "    'ss__with_mean': [False],\n",
    "    'knn__n_neighbors':[3,5,7,9]}\n",
    "\n",
    "knn_gs_tf = GridSearchCV(knn_pipe_tf, tfidf_params, n_jobs = -1, cv = 5, verbose = 1)\n",
    "knn_gs_tf.fit(X_train, y_train)\n",
    "knn_predict_tf = knn_gs_tf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTfidf vectorizer with K Nearest Neighbors\n",
      "\u001b[0mSpecificity of model\t\t: 0.5764525993883792\n",
      "Sensitivity of model\t\t: 0.7740963855421686\n",
      "Accuracy for the model\t\t: 0.6496025971923703\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.58      0.64       654\n",
      "           1       0.65      0.77      0.71       664\n",
      "\n",
      "    accuracy                           0.68      1318\n",
      "   macro avg       0.68      0.68      0.67      1318\n",
      "weighted avg       0.68      0.68      0.67      1318\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Basketball</th>\n",
       "      <th>Predicted Soccer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual Basketball</th>\n",
       "      <td>377</td>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual Soccer</th>\n",
       "      <td>150</td>\n",
       "      <td>514</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Predicted Basketball  Predicted Soccer\n",
       "Actual Basketball                   377               277\n",
       "Actual Soccer                       150               514"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TN, FP, FN, TP = confusion_matrix(y_test,knn_predict_tf).ravel()\n",
    "\n",
    "print('\\033[1m'+'Tfidf vectorizer with K Nearest Neighbors')\n",
    "print('\\033[0m'f'Specificity of model\\t\\t: {TN/(TN + FP)}')\n",
    "print(f'Sensitivity of model\\t\\t: {TP/(TP + FN)}')\n",
    "print(f'Accuracy for the model\\t\\t: {knn_gs_tf.best_score_}\\n')\n",
    "print(classification_report(y_test,knn_predict_tf))\n",
    "\n",
    "pd.DataFrame(confusion_matrix(y_test,knn_predict_tf),\n",
    "             index = ['Actual Basketball', 'Actual Soccer'],\n",
    "             columns = ['Predicted Basketball','Predicted Soccer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compared between Count Vectorizer and Tfidf Vectorizer and tuned its parameters using Grid Search to build our K Nearest Neighbors model. Both results have a higher specificity than sensitivity. This shows that our K Nearest Neighbors models are better at predicting texts that are actually basketball than texts that are actually soccer. Also, we found that K Nearest Neighbors with Tfidf Vectorizer produces a better result than the K Nearest Neighbors with Count Vectorizer. In comparison to our benchmark model, both K Nearest Neighbors models performed better in terms of Sensitivity and Accuracy than our benchmarks. However, specificity of both K Nearest Neighbors models did not out perform our benchmark model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations of Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is fitted with sentences that were scrapped from basketball and soccer reddit APIs. As such, the model analysis is limited to the corpus of texts obtained.\n",
    "\n",
    "Any words that are new to the corpus will not be considered when doing vectorizing transformation and prediction. At the same time, the API has a maximum capacity. So the data that we obtained are limited by the this cap.\n",
    "\n",
    "Our logistic model assumes linear separability between different texts. However, in reality, texts or comments are not exactly linearly separable. At the same time, Naive Bayes model assumes independence between features. Texts or comments may not be independent in fact\n",
    "\n",
    "Our models are only applicable to analyse basketball and soccer sentences. If a sentence is related to other sports, for example american football, our model might predict the text being associated to soccer\n",
    "\n",
    "We are trying to classify between soccer texts and basketball texts this is because of the nature of MLS business. If MLS wants to compare between other sports, all we have to do is to repeat the process by getting the data we want, retrain our model and we will be able to get the model we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Recommendations\n",
    "**Production Model and Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_predict = mnb_gs_tf.predict(X_unseen) # Predicting unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mProduction Model and implementation on unseen date\n",
      "\u001b[0mSpecificity of model\t\t: 0.9253365973072215\n",
      "Sensitivity of model\t\t: 0.8736462093862816\n",
      "Accuracy for the model\t\t: 0.8992718446601942\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.93      0.90       817\n",
      "           1       0.92      0.87      0.90       831\n",
      "\n",
      "    accuracy                           0.90      1648\n",
      "   macro avg       0.90      0.90      0.90      1648\n",
      "weighted avg       0.90      0.90      0.90      1648\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Basketball</th>\n",
       "      <th>Predicted Soccer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual Basketball</th>\n",
       "      <td>756</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual Soccer</th>\n",
       "      <td>105</td>\n",
       "      <td>726</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Predicted Basketball  Predicted Soccer\n",
       "Actual Basketball                   756                61\n",
       "Actual Soccer                       105               726"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TN, FP, FN, TP = confusion_matrix(y_unseen,unseen_predict).ravel()\n",
    "\n",
    "print('\\033[1m'+'Production Model and implementation on unseen date')\n",
    "print('\\033[0m'f'Specificity of model\\t\\t: {TN/(TN + FP)}')\n",
    "print(f'Sensitivity of model\\t\\t: {TP/(TP + FN)}')\n",
    "print(f'Accuracy for the model\\t\\t: {mnb_gs_tf.score(X_unseen, y_unseen)}\\n')\n",
    "print(classification_report(y_unseen, unseen_predict))\n",
    "\n",
    "pd.DataFrame(confusion_matrix(y_unseen, unseen_predict),\n",
    "             index = ['Actual Basketball', 'Actual Soccer'],\n",
    "             columns = ['Predicted Basketball','Predicted Soccer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soccer_or_basketball(sentence):   # Function to predict if text is soccer or basketball\n",
    "    if mnb_gs_tf.predict(sentence) == 1:\n",
    "        return('Text associated to Soccer')\n",
    "    \n",
    "    else:\n",
    "        return('Text associated to Basketball')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bringing the entertainment closer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Text associated to Basketball'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soccer_or_basketball([str(input())]) # Example one : 'Bringing the entertainment closer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expansion plans coming your way\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Text associated to Soccer'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soccer_or_basketball([str(input())]) # Example two : 'Expansion plans coming your way'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bringing more stadiums to you\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Text associated to Soccer'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soccer_or_basketball([str(input())]) # Example three : 'Bringing more stadiums to you'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting the most from 90 minutes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Text associated to Soccer'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soccer_or_basketball([str(input())]) # Example four : 'Getting the most from 90 minutes'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We selected the Multinomial Naive Bayes with Tfidf Vectorizer as our production model due to the high specificity and accuracy. Remember that we want to optimize the specificity of our model as we want to filter out all texts that are associated to basketball. Having said that, we want a model that gives us minimum false positives given that soccer is 1 while basketball is 0.\n",
    "\n",
    "By implementing our model on unseen data, we yield the following results :\n",
    "\n",
    "||Production Model|Benchmark Model|\n",
    "|---|---|---|\n",
    "|**Specificity**|0.92355|0.62385|\n",
    "|**Accuracy**|0.8994|0.63281|\n",
    "|**Sensitivity**|0.87801|0.86750|\n",
    "\n",
    "\\\n",
    "Even with unseen data, our production model outperformed the benchmark model in specificity, accuracy as well as sensitivity. This shows that our model is able to tell whether a statement is associated to soccer or basketball and can help in evaluating any suggested slogans.\n",
    "\n",
    "We included a few sample slogans above to show you how our model evaluates them. The first sample is associated to basketball while the next few are associated to soccer.\n",
    "\n",
    "By adopting our model, MLS will be able to consolidate all suggested slogans and run them through our model for evaluation. You will be able to filter out the slogans that are predicted as basketball and that will very much narrow down and reduce your search for a suitable slogan.\n",
    "\n",
    "We believe the model we developed is capable of solving the issue of evaluating whether a suggested slogan is associated to basketball or soccer. Besides the slogan, this model is not subjected to a one time usage. You can implement it for any future testing of sentences and texts in your marketing materials or campaigns.\n",
    "\n",
    "Though our model maybe useful at filtering out basketball texts, it is recommended that apart from solely relying on our model, you exercise your domain expertise and intuition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for selecting words that you are considering in the slogan, we recommend MLS to adopt the Tfidf vectorizer with logistic model's results for highly effective words for soccer and avoid words that are well associated to basketball.\n",
    "\n",
    "We selected this logistic model to select keywords as it is also highly accurate and has better specificity, accuracy and sensitivity than our benchmark.\n",
    "\n",
    "||Keywords Model|Benchmark Model|\n",
    "|---|---|---|\n",
    "|**Specificity**|0.84404|0.62385|\n",
    "|**Accuracy**|0.89110|0.63281|\n",
    "|**Sensitivity**|0.94277|0.86750|\n",
    "\n",
    "Below are the words we believe are highly associated to soccer and basketball respectively. They are the highest/lowest coefficient words from our logistic model with Tfidf vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words likely related to soccer\t:\n",
      "            Coefficients\n",
      "sign           3.011367\n",
      "footbal        2.493096\n",
      "match          2.488996\n",
      "season         2.219935\n",
      "goal           2.196048\n",
      "unit           2.188189\n",
      "bayern         2.072980\n",
      "loan           1.903082\n",
      "arsen          1.762674\n",
      "liverpool      1.730885 \n",
      "\n",
      "Words likely related to basketball\t:\n",
      "            Coefficients\n",
      "basketbal     -6.201912\n",
      "nba           -4.471698\n",
      "practic       -3.916437\n",
      "dunk          -3.473481\n",
      "help          -3.392226\n",
      "jump          -3.359188\n",
      "school        -3.164266\n",
      "court         -3.088074\n",
      "work          -3.026016\n",
      "shoe          -2.767702\n"
     ]
    }
   ],
   "source": [
    "print('Words likely related to soccer\\t:\\n',log_tf_coef.sort_values(by='Coefficients', ascending = False)[:10],'\\n')\n",
    "print('Words likely related to basketball\\t:\\n',log_tf_coef.sort_values(by='Coefficients', ascending = True)[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
